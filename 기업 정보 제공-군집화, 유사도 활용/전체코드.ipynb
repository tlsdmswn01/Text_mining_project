{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsdmswn01/Text_mining_project/blob/main/%EA%B8%B0%EC%97%85%20%EC%A0%95%EB%B3%B4%20%EC%A0%9C%EA%B3%B5-%EA%B5%B0%EC%A7%91%ED%99%94%2C%20%EC%9C%A0%EC%82%AC%EB%8F%84%20%ED%99%9C%EC%9A%A9/%EC%A0%84%EC%B2%B4%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUcF2LNanSyY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석\n",
        "!pip install konlpy\n",
        "# 연관 키워드 추출\n",
        "!pip install keybert"
      ],
      "metadata": {
        "id": "4KEgN175qO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keybert import KeyBERT\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "id": "8nHPuyOHq2UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뉴스 기사 불러오기\n",
        "- 빅카인즈에서 데이터 저장(https://www.bigkinds.or.kr/v2/news/index.do)"
      ],
      "metadata": {
        "id": "hdHdrMhXuvPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/Shareddrives/텍스트 마이닝 프로젝트/웹 크롤링 코드 및 데이터/삼성전자_주가_2020_2023.xlsx')\n",
        "\n",
        "df['일자'] = df['일자'].astype('int')\n",
        "df = df[df['일자'] >= 20230501] # 2023년 5월 이후의 데이터만 사용(최근 뉴스 제공 위해)\n",
        "\n",
        "df = df[['일자', '언론사', '제목', '본문', '키워드' ,'URL']]\n",
        "\n",
        "# 제목에서 [], () 안에 있는 내용 지우기\n",
        "df['제목'] = df['제목'].apply(lambda x : re.sub(r'\\[[^]]*\\]', '', x))\n",
        "df['제목'] = df['제목'].apply(lambda x : re.sub(r'\\([^)]*\\)', '', x))\n",
        "\n",
        "# 중복 행 제거\n",
        "df = df.drop_duplicates(subset='제목', keep='first')\n",
        "#df = df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "DwBdY0c3usQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "MOPcq1GnZfxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뉴스 기사 군집화\n",
        "- 비슷한 내용의 다른 기사가 존재하는 것을 확인함\n",
        "- 군집화를 통해 비슷한 내용이면 하나의 기사만 출력"
      ],
      "metadata": {
        "id": "rZQFBK-5wYwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RANKS NL에서 제공하는 한국어 불용어 사전 활용\n",
        "\n",
        "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "stopwords = sum(stopwords, [])\n",
        "\n",
        "with open('nsmc_stopwords.txt', 'wt') as f:\n",
        "    f.write('\\n'.join(stopwords))  # 리스트 -> 문자열"
      ],
      "metadata": {
        "id": "HuZ10dPT-lr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(review):\n",
        "    okt = Okt()\n",
        "\n",
        "    f = open('nsmc_stopwords.txt')\n",
        "    stop_words = f.read().split()\n",
        "\n",
        "    # 1. 한글, 숫자, 공백을 제외한 문자 모두 제거.\n",
        "    review_text = re.sub(\"[^가-힣\\s0-9]\", \"\", review)\n",
        "\n",
        "    # 2. okt 객체를 활용해서 형태소 토큰화 + 품사 태깅 + stemming\n",
        "    word_review = okt.pos(review_text, stem=True)\n",
        "\n",
        "    # 노이즈 & 불용어 제거\n",
        "    word_review = [(token, pos) for token, pos in word_review if not token in stop_words and len(token) > 1]\n",
        "\n",
        "    # 명사, 동사, 형용사 추출\n",
        "    word_review = [token for token, pos in word_review if pos in ['Noun', 'Verb', 'Adjective']]\n",
        "\n",
        "    return word_review"
      ],
      "metadata": {
        "id": "icnBPNWq-lmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['제목']"
      ],
      "metadata": {
        "id": "HlR-8V6eAgl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = list(map(lambda x : preprocessing(x), text))"
      ],
      "metadata": {
        "id": "2dINsVEm-ljl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dummyTokenizer(x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "jD7pXpsi-lgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TfidfVectorizer 객체 생성\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer = dummyTokenizer, preprocessor = dummyTokenizer, max_features=10000)\n",
        "\n",
        "# 텍스트 데이터를 TF-IDF 벡터로 변환\n",
        "tfidf = tfidf_vectorizer.fit_transform(text)\n",
        "\n",
        "# TF-IDF 벡터화된 결과를 배열로 변환\n",
        "tfidf_array = tfidf.toarray()\n",
        "\n",
        "# 단어 목록 확인\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# TF-IDF 벡터화된 결과 및 단어 목록 출력\n",
        "print(\"TfidfVectorized Data:\")\n",
        "print(tfidf_array)\n",
        "print(\"Feature Names:\")\n",
        "print(feature_names_tfidf)"
      ],
      "metadata": {
        "id": "ONGcdDk4usIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps_values = [0.3, 0.5, 0.7]\n",
        "min_samples_values = [1, 3, 5]\n",
        "\n",
        "# 그래프를 그릴 격자 생성\n",
        "fig, axes = plt.subplots(len(eps_values), len(min_samples_values), figsize=(15, 10))\n",
        "\n",
        "for i, eps in enumerate(eps_values):\n",
        "    for j, min_samples in enumerate(min_samples_values):\n",
        "        # DBSCAN 모델 생성\n",
        "        model = DBSCAN(eps=eps, min_samples=min_samples, metric=\"cosine\")\n",
        "        result = model.fit_predict(tfidf_array)\n",
        "\n",
        "        # t-SNE를 사용하여 데이터를 2차원으로 축소\n",
        "        tsne = TSNE(n_components=2, random_state=1004, perplexity=20)\n",
        "        X_tsne = tsne.fit_transform(tfidf_array)\n",
        "\n",
        "        # 군집화 결과와 함께 서브플롯에 그래프 그리기\n",
        "        axes[i, j].scatter(X_tsne[:, 0], X_tsne[:, 1], c=result, cmap='jet')\n",
        "        axes[i, j].set_title(f'eps={eps}, min_samples={min_samples}')\n",
        "        axes[i, j].axis('off')  # 축 제거\n",
        "\n",
        "# 그래프 출력\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jYa1D7LV49I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN Clustering\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "model = DBSCAN(eps=0.5,min_samples=5, metric = \"cosine\")\n",
        "result = model.fit_predict(tfidf_array)\n",
        "\n",
        "cluster_labels = result"
      ],
      "metadata": {
        "id": "C26L_5Q2ur-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cluster1st'] = result"
      ],
      "metadata": {
        "id": "BKG7n79_ur76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('군집개수 :', result.max())"
      ],
      "metadata": {
        "id": "JVbvRaY8ur3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = []\n",
        "\n",
        "for cluster_num in set(result):\n",
        "    # -1,0은 노이즈 판별이 났거나 클러스터링이 안된 경우\n",
        "    if(cluster_num == -1 or cluster_num == 0):\n",
        "        continue\n",
        "    else:\n",
        "        print(\"cluster num : {}\".format(cluster_num))\n",
        "        temp_df = df[df['cluster1st'] == cluster_num] # cluster num 별로 조회\n",
        "        clusters.append(cluster_num)\n",
        "        for title in temp_df['제목']:\n",
        "            print(title)\n",
        "        print()"
      ],
      "metadata": {
        "id": "keozPJ-LB_Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clu_df = df[(df['cluster1st'] != 0) & (df['cluster1st'] != -1)]"
      ],
      "metadata": {
        "id": "jSWfVjmSB-9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(clu_df['cluster1st'].unique())"
      ],
      "metadata": {
        "id": "CH_H57afF9e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "n = len(clu_df['cluster1st'].unique())\n",
        "for i in range(1, n + 1):\n",
        "    df_list.append(clu_df[clu_df['cluster1st'] == i])"
      ],
      "metadata": {
        "id": "_pRfcsAgDcf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_list[0]"
      ],
      "metadata": {
        "id": "bwE4npe0FdRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 코사인 유사도(메인 기사)"
      ],
      "metadata": {
        "id": "81bL4Sd6ogib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 각 데이터프레임에 대해 정리\n",
        "for i in range(len(df_list)):\n",
        "    dfn = df_list[i]\n",
        "    dfnk = dfn['키워드']\n",
        "\n",
        "    # TF-IDF 벡터화 객체 생성 및 학습\n",
        "    tfidf_vec = TfidfVectorizer(stop_words = 'english')\n",
        "    vec = tfidf_vec.fit_transform(dfnk)\n",
        "\n",
        "    best = []\n",
        "\n",
        "    # 각 행에 대한 코사인 유사도 계산\n",
        "    for k in range(dfnk.count()):\n",
        "        all_cos_sim = cosine_similarity(vec[k],vec)\n",
        "        all_cos_sim = np.delete(all_cos_sim, k, axis=1)\n",
        "        sum_1 = np.sum(all_cos_sim)\n",
        "        best.append(sum_1)\n",
        "\n",
        "    # 'sim'열에 유사도 합계 추가\n",
        "    df_list[i]['sim'] = best"
      ],
      "metadata": {
        "id": "D-Zg9fN-EnRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = []  # '제목' 정보를 저장할 리스트\n",
        "url = []    # 'URL' 정보를 저장할 리스트\n",
        "doc = []    # '키워드' 정보를 저장할 리스트\n",
        "\n",
        "# 각 데이터프레임에 대해 처리\n",
        "for i in range(len(df_list)):\n",
        "    df_result = df_list[i].sort_values(by='sim',ascending=False)\n",
        "    top_entry = df_result.iloc[0, :] # 유사도가 높은 항목 선택\n",
        "\n",
        "    print(top_entry['제목'])\n",
        "    print(top_entry['URL'])\n",
        "    print('==============================================')\n",
        "\n",
        "    title.append(df_result.iloc[0,:]['제목'])\n",
        "    doc.append(df_result.iloc[0,:]['키워드'])\n",
        "    url.append(df_result.iloc[0,:]['URL'])"
      ],
      "metadata": {
        "id": "bS9dTHqBE5Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 집단내에서 cosine 유사도가 가장 높은 기사 하나만 선택 -> dataframe형태\n",
        "main_report=pd.DataFrame({'title':title,'key':doc,'url':url})\n",
        "main_report"
      ],
      "metadata": {
        "id": "975x7jpmE9gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자카드 유사도(유사 사업 기업)"
      ],
      "metadata": {
        "id": "BCYT-K6OolkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/Shareddrives/텍스트 마이닝 프로젝트/웹 크롤링 코드 및 데이터/삼성전자 사업개요.pkl\",\"rb\") as f:\n",
        "    samsung_load = pickle.load(f)\n",
        "samsung_load.append('삼성')\n",
        "samsung_load.append('전자')\n",
        "samsung_load.append('삼성전자')"
      ],
      "metadata": {
        "id": "z3V2pLm7os2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsung_load"
      ],
      "metadata": {
        "id": "xICtQzhkoszJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsung_load.count('삼성')"
      ],
      "metadata": {
        "id": "uLdLwaRVozeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_sim(d1,d2):\n",
        "    s1=set(d1)\n",
        "    s2=set(d2)\n",
        "    return float(len(s1.intersection(s2))/len(s1.union(s2)))\n"
      ],
      "metadata": {
        "id": "5JqarLz3ozax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfnk=main_report['key']\n",
        "jaccard_sim(dfnk.iloc[0].split(','),samsung_load)"
      ],
      "metadata": {
        "id": "AKnz7qbgozYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best=[]\n",
        "for k in range(dfnk.count()):\n",
        "    best.append(jaccard_sim(dfnk.iloc[k].split(','),samsung_load))\n",
        "main_report['sim']=best"
      ],
      "metadata": {
        "id": "P1XmwqFdozWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_report=main_report.sort_values(by='sim',ascending=False)\n",
        "main_report.loc[:10,['url','key']]"
      ],
      "metadata": {
        "id": "khImB7RsozUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_report.to_csv('메인기사 10개 추출.csv')"
      ],
      "metadata": {
        "id": "3n_uEKMkozSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DfGQvJnT8fXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 연관 키워드 추출"
      ],
      "metadata": {
        "id": "z3_TIlm8tyP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BERT(title, num):\n",
        "    # 특정 군집 번호에 해당하는 '키워드' 열 데이터 추출\n",
        "    array_text = pd.DataFrame(df[df['cluster1st'] == num]['키워드']).to_numpy()\n",
        "\n",
        "    bow = []\n",
        "    from keybert import KeyBERT\n",
        "    kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "    # 각 문장에 대한 키워드 추출\n",
        "    for j in range(len(array_text)):\n",
        "        keywords = kw_extractor.extract_keywords(array_text[j][0])\n",
        "        bow.append(keywords)\n",
        "\n",
        "    new_bow = []\n",
        "    for i in range(0, len(bow)):\n",
        "        for j in range(len(bow[i])):\n",
        "            new_bow.append(bow[i][j])\n",
        "\n",
        "    keyword = pd.DataFrame(new_bow, columns=['keyword', 'weight'])\n",
        "    keyword = keyword.groupby('keyword').agg('sum').sort_values('weight', ascending=False).head(20)\n",
        "    return keyword"
      ],
      "metadata": {
        "id": "nzOouqinE9cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_df = pd.DataFrame()\n",
        "cluster_list = []\n",
        "\n",
        "for i in range(1, 15):  # 군집 -1, 0은 오분류, 이상치로 판단\n",
        "    cluster_df = BERT('삼성전자', i)\n",
        "    cluster_list.append(cluster_df)\n",
        "\n",
        "# index 맞추기 위해\n",
        "for i in range(len(cluster_list)):\n",
        "    cluster_list[i] = cluster_list[i].reset_index()\n",
        "\n",
        "# 각 군집의 결과를 하나의 데이터프레임으로 합치고 가중치 순으로 정렬\n",
        "weight_df = pd.concat(cluster_list, axis=0)\n",
        "weight_df = weight_df.sort_values(by='weight', ascending=False)\n",
        "\n",
        "#weight_df.to_csv('/content/drive/Shareddrives/텍스트 마이닝 프로젝트/웹 크롤링 코드 및 데이터/weight_df.csv', index=False)\n",
        "\n",
        "# '삼성전자' 키워드 제외\n",
        "weight_df = weight_df[weight_df['keyword'] != '삼성전자']\n",
        "weight_df"
      ],
      "metadata": {
        "id": "R9gm7v0dt_48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUkoZIU8oT1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0s2EkBXAoTyt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}