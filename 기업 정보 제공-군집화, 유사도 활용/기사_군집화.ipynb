{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsdmswn01/Text_mining_project/blob/main/%EA%B8%B0%EC%97%85%20%EC%A0%95%EB%B3%B4%20%EC%A0%9C%EA%B3%B5-%EA%B5%B0%EC%A7%91%ED%99%94%2C%20%EC%9C%A0%EC%82%AC%EB%8F%84%20%ED%99%9C%EC%9A%A9/%EA%B8%B0%EC%82%AC_%EA%B5%B0%EC%A7%91%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 뉴스[제목]으로 비슷한 뉴스 군집화"
      ],
      "metadata": {
        "id": "5bH6qDpB9W76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IERerQVm7kHC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel('삼성전자_주가_2020_2023.xlsx')\n",
        "\n",
        "df['일자'] = df['일자'].astype('int')\n",
        "df = df[df['일자'] >= 20230501] # 2023년 5월 이후의 데이터만 사용(최근 뉴스 제공 위해)\n",
        "\n",
        "df = df[['일자', '언론사', '제목', '본문', '키워드' ,'URL']]\n",
        "\n",
        "# 제목에서 [], () 안에 있는 내용 지우기\n",
        "df['제목'] = df['제목'].apply(lambda x : re.sub(r'\\[[^]]*\\]', '', x))\n",
        "df['제목'] = df['제목'].apply(lambda x : re.sub(r'\\([^)]*\\)', '', x))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt() # 형태소 분석기 객체 생성\n",
        "noun_list = []\n",
        "for content in df['제목']:\n",
        "    nouns = okt.nouns(content) # 명사만 추출하기, 결과값은 명사 리스트\n",
        "    noun_list.append(nouns)\n",
        "\n",
        "df['nouns'] = noun_list  # 제목에서 추출한 명사\n",
        "\n",
        "drop_index_list = [] # 지워버릴 index를 담는 리스트\n",
        "for i, row in df.iterrows():\n",
        "    temp_nouns = row['nouns']\n",
        "    if len(temp_nouns) == 0: # 만약 명사리스트가 비어 있다면\n",
        "        drop_index_list.append(i) # 지울 index 추가\n",
        "\n",
        "df = df.drop(drop_index_list) # 해당 index를 지우기\n",
        "\n",
        "# index를 지우면 순회시 index 값이 중간중간 비기 때문에 index를 다시 지정\n",
        "df.index = range(len(df))"
      ],
      "metadata": {
        "id": "EbCaxKov7qbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 문서를 명사 집합으로 보고 문서 리스트로 치환 (tfidfVectorizer 인풋 형태를 맞추기 위해)\n",
        "text = [\" \".join(noun) for noun in df['nouns']]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df = 5, ngram_range=(1,5))\n",
        "tfidf_vectorizer.fit(text)\n",
        "vector = tfidf_vectorizer.transform(text).toarray()\n",
        "vector = np.array(vector) # Normalizer를 이용해 변환된 벡터"
      ],
      "metadata": {
        "id": "l9hrf7p48TV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN Clustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "model = DBSCAN(eps=0.3,min_samples=5, metric = \"cosine\")\n",
        "# 거리 계산 식으로는 Cosine distance를 이용\n",
        "result = model.fit_predict(vector)\n",
        "df['cluster1st'] = result\n",
        "\n",
        "print('군집개수 :', result.max())\n",
        "df"
      ],
      "metadata": {
        "id": "uELIBTMu8Vmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 기준으로 각 cluster group 중 하나의 기사를 뽑고\n",
        "# 각 그룹에서 빈도 높은 키워드 뽑음\n",
        "\n",
        "clusters = []\n",
        "counts = []\n",
        "top_title = []\n",
        "top_noun = []\n",
        "for cluster_num in set(result):\n",
        "    # -1,0은 노이즈 판별이 났거나 클러스터링이 안된 경우\n",
        "    # if(cluster_num == -1 or cluster_num == 0):\n",
        "    #     continue\n",
        "    # else:\n",
        "        print(\"cluster num : {}\".format(cluster_num))\n",
        "        temp_df = df[df['cluster1st'] == cluster_num] # cluster num 별로 조회\n",
        "        clusters.append(cluster_num)\n",
        "        counts.append(len(temp_df))\n",
        "        top_title.append(temp_df.reset_index()['제목'][0])\n",
        "        top_noun.append(temp_df.reset_index()['nouns'][0]) # 군집별 첫번째 기사를 대표기사로 ; tfidf방식\n",
        "        for title in temp_df['제목']:\n",
        "            print(title) # 제목으로 살펴보자\n",
        "        print()\n",
        "\n",
        "cluster_result = pd.DataFrame({'cluster_num':clusters, 'count':counts, 'top_title':top_title, 'top_noun':top_noun})\n",
        "cluster_result"
      ],
      "metadata": {
        "id": "F2z8sBk98YBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 본문[키워드]로 군집별 키워드 뽑기"
      ],
      "metadata": {
        "id": "kFaIvKaP9ApL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "1Nlif3gy8_gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "def BERT(title):\n",
        "    # 1번째 군집\n",
        "    array_text = pd.DataFrame(df[df['cluster1st'] == 1]['키워드']).to_numpy()\n",
        "\n",
        "    bow = []\n",
        "    from keybert import KeyBERT\n",
        "    kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
        "    for j in range(len(array_text)):\n",
        "        keywords = kw_extractor.extract_keywords(array_text[j][0])\n",
        "        bow.append(keywords)\n",
        "\n",
        "    new_bow = []\n",
        "    for i in range(0, len(bow)):\n",
        "        for j in range(len(bow[i])):\n",
        "            new_bow.append(bow[i][j])\n",
        "\n",
        "    keyword = pd.DataFrame(new_bow, columns=['keyword', 'weight'])\n",
        "    print(keyword.groupby('keyword').agg('sum').sort_values('weight', ascending=False).head(20))"
      ],
      "metadata": {
        "id": "LLFNTuAb9JG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT('삼성전자')  # '삼성전자'와 가장 관련이 높은 키워드"
      ],
      "metadata": {
        "id": "n8bd0MiD9Ljd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}